{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FI_estimation\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from scipy import ndimage\n",
    "\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_tr shape:  (620000, 1, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "# ---------- load data and prepare training and testing data sets\n",
    "speckles = np.load('Data'+os.sep+'normal_data.npy').astype(type)\n",
    "positions = speckles.shape[0]\n",
    "size = speckles.shape[1]\n",
    "N_sp = positions*size\n",
    "N_pix = speckles.shape[2]\n",
    "pos = np.zeros(shape = N_sp)\n",
    "for i in range(N_sp):\n",
    "    pos[i] = float(i*speckles.shape[1])//(N_sp)\n",
    "pos = pos/(speckles.shape[1]-1)\n",
    "speckles = speckles.reshape(N_sp,N_pix,N_pix)\n",
    "\n",
    "x_tr = np.zeros((N_sp,N_pix,N_pix))\n",
    "y_tr = np.zeros((N_sp))\n",
    "for k in range(N_sp):\n",
    "    tmps = speckles[k,:,:]\n",
    "    x_tr[k,:,:] = tmps\n",
    "    y_tr[k] = pos[k]\n",
    "x_tr = x_tr.reshape(N_sp, 1, N_pix,N_pix)\n",
    "print(\"x_tr shape: \",x_tr.shape)\n",
    "    \n",
    "n_samples, n_channels, dim_x, dim_y = x_tr.shape\n",
    "def shuffle_split_data(X, y, ratio=0.5):\n",
    "    arr_rand = np.random.rand(X.shape[0])\n",
    "    split = arr_rand < np.percentile(arr_rand, ratio*100)\n",
    "    X_train = X[split]\n",
    "    y_train = y[split]\n",
    "    X_test =  X[~split]\n",
    "    y_test = y[~split]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_val, Y_train, Y_val = shuffle_split_data(x_tr, y_tr, ratio = 0.8)\n",
    "type = np.float64\n",
    "X_train = X_train.astype(type)\n",
    "Y_train = Y_train.astype(type)\n",
    "X_val = X_val.astype(type)\n",
    "Y_val = Y_val.astype(type)\n",
    "x_tst = np.load(\"Data\"+os.sep+\"normal_data_test.npy\")\n",
    "delta = 0.03 # ---------- step size for calculating the FI\n",
    "# ---------- standardize training and testing data\n",
    "a = x_tr.shape[0]\n",
    "std = np.std(x_tr, axis=0)\n",
    "mean = np.mean(x_tr, axis=0)\n",
    "for i in range(a): \n",
    "    x_tr[i] = (x_tr[i]-mean)/std\n",
    "for i in range(x_tst.shape[0]):\n",
    "    x_tst[i] = (x_tst[i]-mean[0])/std[0]\n",
    "x_tst = x_tst.astype(np.float64)\n",
    "x_minus = x_tst[0]\n",
    "x_middle = x_tst[1]\n",
    "x_plus = x_tst[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply the output variance with 1.0\n"
     ]
    }
   ],
   "source": [
    "# ---------- rescale the targets\n",
    "cf = 4 * (np.max(y_tr)/2)**2\n",
    "print(\"multiply the output variance with\", cf)\n",
    "y_tr = y_tr-np.min(y_tr)\n",
    "y_tr = 2 * ( y_tr / np.max(y_tr) - 0.5 )\n",
    "X_train, X_val, Y_train, Y_val = shuffle_split_data(x_tr, y_tr, ratio=0.8)\n",
    "x_tr = None\n",
    "y_tr = None\n",
    "X_train = X_train.astype(type)\n",
    "Y_train = Y_train.astype(type)\n",
    "X_val = X_val.astype(type)\n",
    "Y_val = Y_val.astype(type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- data loader\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "train_set = TensorDataset(torch.tensor(X_train), torch.tensor(Y_train).T)\n",
    "val_set = TensorDataset(torch.tensor(X_val), torch.tensor(Y_val).T)\n",
    "bs = 128\n",
    "train_loader = Data.DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "validation_loader = Data.DataLoader(val_set, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(n[0], n[1])\n",
    "        self.linear2 = nn.Linear(n[1], n[2])\n",
    "        self.linear3 = nn.Linear(n[2], n[3])\n",
    "        self.linear4 = nn.Linear(n[3], n[4])\n",
    "        self.linear5 = nn.Linear(n[4], n[5])\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)    \n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear4(x)       \n",
    "        x = F.relu(x)\n",
    "        x = self.linear5(x)  \n",
    "        return x\n",
    "n0 = 256\n",
    "n = [n0,150,100,50,25,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_epoch = 0\n",
    "criterion =  nn.MSELoss()\n",
    "n_epochs = 40\n",
    "lr = 1e-6\n",
    "training_loss = 0.\n",
    "validation_loss = 0.\n",
    "net = Model(n).to(device)\n",
    "all_losses = []\n",
    "all_losses_val = []\n",
    "all_losses.append(training_loss/len(train_loader))\n",
    "all_losses_val.append(validation_loss/len(validation_loader))\n",
    "#torch.save(net,  'Models' +os.sep+ 'epoch' + str(0)+'Gaussian.pth')\n",
    "for epoch in range(starting_epoch, starting_epoch + n_epochs):\n",
    "    #\n",
    "    with torch.no_grad():\n",
    "        ypr_minus = net.to('cpu')(torch.tensor(x_minus))\n",
    "        ypr_middle = net.to('cpu')(torch.tensor(x_middle))\n",
    "        ypr_plus = net.to('cpu')(torch.tensor(x_plus))\n",
    "        lfi = ((torch.mean(ypr_plus)-torch.mean(ypr_minus))/(2*delta))**2/((torch.var(ypr_plus)+torch.var(ypr_minus))/2)\n",
    "        print(\"lfi: \", lfi)\n",
    "    #\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.003)\n",
    "    net.train()\n",
    "    training_loss = 0.\n",
    "    validation_loss = 0.\n",
    "    batch_count = 0\n",
    "    for inputs, target in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_count += 1\n",
    "        y_pred = net(inputs)\n",
    "        loss = criterion(y_pred,  torch.unsqueeze(target,dim=1)  )\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in validation_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            y_pred = net(inputs)\n",
    "            loss_val = criterion(y_pred,  torch.unsqueeze(target,dim=1) )\n",
    "            validation_loss += loss_val.item()\n",
    "    print(\"epoch= \", epoch, \", val= \", validation_loss/len(validation_loader),\", train = \", training_loss/len(train_loader))\n",
    "#    torch.save(net,   'Models' +os.sep+ 'epoch' + str(epoch+1)+'Gaussian.pth')\n",
    "    all_losses_val.append(validation_loss/len(validation_loader))\n",
    "    all_losses.append(training_loss/len(train_loader))\n",
    "# np.savez('Plots'+os.sep+'losses_normal.npz',\n",
    "#          all_losses=all_losses,\n",
    "#         all_losses_val = all_losses_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FI based early stopping, increase the size of the network and decrease the learning rate\n",
    "If y_tr is rescaled then the MSE must be rescaled accordingly in order to be comparable with 1/FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------- calculating the FI flow at different epochs\n",
    "#------- parameters entering the algorithm; they should be adjusted if convergence is not reached\n",
    "fi_in = 256\n",
    "kappa = 0.1 # ratio of slopes\n",
    "gamma = 0.05 # stop when the LFI doesn't improve in the first iterations \n",
    "noise = 0.01 # noise added\n",
    "step = 30 # 10 increase of dimensionality\n",
    "upperLimit = 500 # maximum increase \n",
    "n_epochs = 20\n",
    "X = x_tst\n",
    "X = X.reshape(X.shape[0]*X.shape[1], X.shape[2]*X.shape[3]) \n",
    "X = torch.tensor(X)\n",
    "print(\"X.shape\", X.shape)\n",
    "fi_array = []\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"epoch: \", epoch)\n",
    "    net = Model(n)\n",
    "    net = torch.load('Models'+os.sep + 'epoch'+ str(epoch)+'Gaussian.pth', map_location=torch.device('cpu'))\n",
    "    net.eval()\n",
    "    # ------- extract data arrays\n",
    "    return_nodes = {\n",
    "        \"linear1\": \"linear1\",\n",
    "        \"linear2\": \"linear2\",\n",
    "        \"linear3\": \"linear3\",\n",
    "        \"linear4\": \"linear4\"}\n",
    "    model2 = create_feature_extractor(net, return_nodes=return_nodes)\n",
    "    intermediate_outputs = model2(X)\n",
    "    linear1 = intermediate_outputs['linear1']\n",
    "    linear2 = intermediate_outputs['linear2']\n",
    "    linear3 = intermediate_outputs['linear3']\n",
    "    linear4 = intermediate_outputs['linear4']\n",
    "    layer_out = net(X)\n",
    "    # ------- reshape arrays\n",
    "    linear1 = linear1.reshape(3 , linear1.shape[0]//3, linear1.shape[1])\n",
    "    linear2 = linear2.reshape(3 , linear2.shape[0]//3, linear2.shape[1])\n",
    "    linear3 = linear3.reshape(3 , linear3.shape[0]//3, linear3.shape[1])\n",
    "    linear4 = linear4.reshape(3 , linear4.shape[0]//3, linear4.shape[1])\n",
    "    layer_out = layer_out.reshape(3 , layer_out.shape[0]//3, layer_out.shape[1])\n",
    "    # ------- convert to numpy arrays\n",
    "    linear1 = linear1.detach().numpy()\n",
    "    linear2 = linear2.detach().numpy()\n",
    "    linear3 = linear3.detach().numpy()\n",
    "    linear4 = linear4.detach().numpy()\n",
    "    layer_out = layer_out.detach().numpy()\n",
    "    # ------- apply activation function since the data is extracted directly after the linear transformation\n",
    "    linear1 = (FI_estimation.ReLU(linear1))\n",
    "    linear2 = (FI_estimation.ReLU(linear2))\n",
    "    linear3 = (FI_estimation.ReLU(linear3))\n",
    "    linear4 = (FI_estimation.ReLU(linear4))\n",
    "    layer_out = (layer_out)\n",
    "    # ------- calculate the FI\n",
    "    fi_1 = FI_estimation.get_FI(linear1, delta, upperLimit, step, kappa, constant_threshold=gamma, noise_factor=noise, biasedLFI=True)\n",
    "    fi_2 = FI_estimation.get_FI(linear2, delta, upperLimit, step, kappa, constant_threshold=gamma, noise_factor=noise, biasedLFI=True)\n",
    "    fi_3 = FI_estimation.get_FI(linear3, delta, upperLimit, step, kappa, constant_threshold=gamma, noise_factor=noise, biasedLFI=True)\n",
    "    fi_4 = FI_estimation.get_FI(linear4, delta, upperLimit, step, kappa, constant_threshold=gamma, noise_factor=noise, biasedLFI=True)\n",
    "    fi_out = FI_estimation.get_FI(layer_out, delta, upperLimit, step, kappa, constant_threshold=gamma, noise_factor=noise, biasedLFI=True)\n",
    "    fi = np.array([fi_in,fi_1,fi_2,fi_3,fi_4,fi_out])\n",
    "    print(fi)\n",
    "    fi_array.append(fi)\n",
    "    print(\"#------------------------------------------------------#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(\"Plots\" + os.sep + \"fi_flow_Normal.npy\", np.array(fi_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
